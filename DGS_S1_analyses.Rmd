---
title: "DGS_S1_analyses"
author: "Qilin Zhang"
date: "2023-02-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

###load packages

```{r pac, message = FALSE}
library(summarytools)
library(tidyverse) #data wrangling
library(codebook) #codebook generation
library(future) #reliability
library(ufs) #reliability
library(GGally) #reliability
library(GPArotation) #reliability
library(rio) #reading in different file types
library(labelled) #labeling data
library(psych)
library(corrplot)
library("psych")
library(epmr)
library(mirt)
library(reshape2)
library(eRm)
library(mice)

#if need to download the epmr package
#if (!require("devtools")) install.packages("devtools")   
#devtools::install_github("talbano/epmr")
```

### cleaning


```{r cleaning code, include = FALSE}

## use other read functions as appropriate for file type

dict <- rio::import(file = "data/DGS_S1_dictionary.xlsx") #dictionary

data <- read.csv(file = 'data/DGS_S1_Deidentified For Analyses.csv', sep = ",") #data

data <- data[-c(1:2),-c(1:7)]

## Variable types 
names <- dict %>% 
  filter(type == "character") %>% 
  pull(variable)
data[,names] <- 
  lapply(data[,names], as.character)

names <- dict %>% 
  filter(type == "factor") %>% 
  pull(variable)
data[,names] <- 
  lapply(data[,names], as.numeric) #factor variables are coded as numeric for codebook purposes

names <- dict %>% 
  filter(type == "numeric") %>% 
  pull(variable)
data[,names] <- 
  lapply(data[,names], as.numeric)

rm(names)

##data completion check and imputation

#remove failed attention check -55 participants from this process
data <- data %>%
  filter(DGS_31 == 4) %>%
  filter((DGS_53 == 2))


#function for checking percentage of missing data (unit=%)
percent_missing <- function(x){
  sum(is.na((x))/length(x)*100)
}
missing_R <- apply(data,1,percent_missing)
table(missing_R)

#subset based on filtering criteria
replace_rows <- subset(data, missing_R<=10)
no_rows <- subset(data, missing_R>10)

missing_C <- apply(replace_rows,2,percent_missing)
table(missing_C) #no concern here

replace_data <- replace_rows[,1:85]
leftout <- replace_rows[,86:91]

#check where the NAs are if needed
rindex <- rep(FALSE, nrow(replace_data))
for (i in 1:nrow(replace_data)){
  for (j in 1:grep("DGS_75",colnames(replace_data))){
    if( is.na(replace_data[i,j])){
      rindex[i] = TRUE
      j = ncol(replace_data)+1
    }
  }
}
data_error <- replace_data[rindex,]

rm(data_error)
#imputation
temp <- mice(replace_data)
fixed_data <- complete(temp)  #imputation using mice package
data <- cbind(fixed_data,leftout) #no additional participants were removed

rm(fixed_data,leftout,no_rows,replace_data,replace_rows)

##recode

#DGS recode
likert <- dict %>% 
  filter (value_label == "1 = Strongly disagree, 2 = Disagree, 3 = Somewhat disagree, 4 = Neither agree nor disagree, 5 = Somewhat agree, 6 = Agree, 7 = Strongly agree") %>%
  pull(variable)
add_likert <- function(x) {
  val_labels(x) <- c("Strongly disagree"= 1, "Disagree" = 2, "Somewhat disagree" = 3, "Neither agree nor disagree" = 4, "Somewhat agree" = 5, "Agree" = 6, "Strongly agree" = 7) 
  x
}
data <- data %>%
  mutate_at(likert, 
            add_likert)  

rm(likert, add_likert)

#HEXACO_C

likert <- dict %>% 
  filter (value_label == "1 = Strongly disagree, 2 = Somewhat disagree, 3 = Neither agree nor disagree, 4 = Somewhat agree, 5 = Strongly agree") %>%
  pull(variable)
add_likert <- function(x) {
  val_labels(x) <- c("Strongly disagree"= 1, "Somewhat disagree" = 2, "Neither agree nor disagree" = 3, "Somewhat agree" = 4, "Strongly agree" = 5) 
  x
}
data <- data %>%
  mutate_at(likert, 
            add_likert)  

rm(likert, add_likert)

## Reverse-scoring 
reversed_items <- dict %>%  #make a list of reversed items
 filter (keying == -1) %>% 
 pull(variable)

data <- data %>%  #reverse values in data
  mutate_at(reversed_items,
          reverse_labelled_values)

rm(reversed_items)

##scale construction

## Variable labels
var_label(data) <- dict %>% 
  select(variable, label) %>% 
  dict_to_list()

rm(extra)

##DGS
DGS <- dict %>% 
  filter (scale == "DGS") %>% 
  pull(variable)

data$DGS <- data %>% 
  select(all_of(DGS)) %>% 
  aggregate_and_document_scale()

##HEXACO_C

HEXACO_C <- dict %>% 
  filter (scale == "HEXACO_C") %>% 
  pull(variable)

data$HEXACO_C <- data %>% 
  select(all_of(HEXACO_C)) %>% 
  aggregate_and_document_scale()

###Conscientiousness 
Organization <- dict %>% 
  filter (subscale == "Organization") %>% 
  pull(variable)

Diligence <- dict %>% 
  filter (subscale == "Diligence") %>% 
  pull(variable)

Perfectionism <- dict %>% 
  filter (subscale == "Perfectionism") %>% 
  pull(variable)

Prudence <- dict %>% 
  filter (subscale == "Prudence") %>% 
  pull(variable)

###Conscientiousness

data$Organization <- data %>% 
  select(all_of(Organization)) %>% 
  aggregate_and_document_scale()

data$Diligence  <- data %>% 
  select(all_of(Diligence)) %>% 
  aggregate_and_document_scale()

data$Perfectionism <- data %>% 
  select(all_of(Perfectionism)) %>% 
  aggregate_and_document_scale()

data$Prudence <- data %>% 
 select(all_of(Prudence)) %>% 
  aggregate_and_document_scale()

## assumption check
random_variable <- rchisq(nrow(data), 7)
fake_model <- lm(random_variable ~ ., 
                 data = data[ , -c(86:91)])
standardized <- rstudent(fake_model)
fitvalues <- scale(fake_model$fitted.values)
plot(fake_model,2)#check for linearity
#We assume the the multivariate relationship between continuous variables is linear (i.e., no curved)
#There are many ways to test this, but we can use a QQ/PP Plot to examine for linearity

hist(standardized)#check for normality
#We expect that the residuals are normally distributed
#Not that the *sample* is normally distributed 
#Generally, SEM requires a large sample size, thus, buffering against normality deviations

{plot(standardized, fitvalues)
  abline(v = 0)
  abline(h = 0)
}#check for homogeneity + Homoscedasticity
#These assumptions are about equality of the variances
#We assume equal variances between groups for things like t-tests, ANOVA
#Here the assumption is equality in the spread of variance across predicted values 

##rename
data_cleaned <- data


```

###factor analyses

```{r EFA}
#preparing scales variables
DGS <- data_cleaned[grep("DGS_1",colnames(data_cleaned)):grep("DGS_75",colnames(data_cleaned))]
DGS <- DGS[,-c(grep("DGS_31",colnames(DGS)),grep("DGS_53",colnames(DGS)))]

HEXACO_C <- data_cleaned[grep("HEXACO_C_1",colnames(data_cleaned)):grep("HEXACO_C_10",colnames(data_cleaned))]

##EFA for DGS
DGS_descriptive <- describe(DGS)

EFA_fit <- fa(DGS,
             nfactors = 7,
             rotate = "promax",
             fm="ml")
EFA_fit
print(EFA_fit$loadings, cutoff = 0.3)
fa.plot(EFA_fit,
  labels = colnames(DGS)
)

fa.diagram(EFA_fit)

##fit indices
EFA_fit$rms  # Root mean square of the residuals (lower the better)
EFA_fit$RMSEA # root mean squared error of approximation (lower the better)
EFA_fit$TLI  # tucker lewis index
1- ((EFA_fit$STATISTIC-EFA_fit$dof)/
      (EFA_fit$null.chisq-EFA_fit$null.dof))  #CFI

##Reliability
psych::alpha(HEXACO_C) #need to debug this function

factanal(as.matrix(DGS), factors =8, rotation = "oblimin")

factanal(as.matrix(DGS), factors =7, rotation = "oblimin",fm="ml")

factanal(as.matrix(DGS), factors =3, rotation = "promax")

factanal(as.matrix(DGS), factors =1, rotation = "promax")

#visualization
model1 <- fastudy(DGS, factors = 1,rotation = "promax")

print(model1,digit = 2, cutoff = 0.40)

model3 <- fastudy(DGS, factors = 2,  rotation = "promax")

print(model3,digit = 2, cutoff = 0.5)

load <- model3$loadings[,1:2]
plot(load,type="n",xlim = c(0.5,1))
text(load,labels=names(DGS),cex=.7)

loads <- model3$loadings
fa.diagram(loads)

model7 <- fastudy(DGS, factors = 7,rotation = "promax")

print(model7,digit = 2, cutoff = 0.40)

plot(model7, ylim = c(0, 7))

load <- model7$loadings[,1:2]
plot(load,type="n",xlim = c(0.5,1))
text(load,labels=names(DGS),cex=.7)


##Kaiser Criterion
ev<- eigen(cor(DGS))
ev$values
sum(ev$values > 1)
sum(ev$values > .7)

#scree plot and parallel analysis
scree(DGS, pc=FALSE)
fa.parallel(DGS,
            fm="ml",
            fa="fa")

test <- psych::omega(DGS,nfactors=3)
print(test,digit = 2, cutoff = 0.40)

## make a correlation matrix to screen for overlapping items
## Pick out 

##we can use corplot
#corrplot(cor(data_cleaned[,-c(86:91)]))
```

###Item analysis

```{r Item_analysis}

describe(DGS)

data_cleaned %>%
  select(DGS_1) %>%
  ggplot(aes(x=DGS_1)) +
  geom_bar()

data_cleaned %>%
  select(DGS_12) %>%
  ggplot(aes(x=DGS_12)) +
  geom_bar()

data_cleaned %>%
  select(DGS_15) %>%
  ggplot(aes(x=DGS_15)) +
  geom_bar()

data_cleaned %>%
  select(DGS_17) %>%
  ggplot(aes(x=DGS_17)) +
  geom_bar()

##make these into a function? 
M7_f7 <- DGS%>%
  select(DGS_9,DGS_29,DGS_52)


## functions for Rating Scale Model
IRT_R <- function(x) {
  M <- x-1
  RSM_M <- RSM(X = M)
  person.parameter_M <- person.parameter(object = RSM_M)
  itemfit_M <- itemfit(object = person.parameter_M)
  thresholds_M <- thresholds(object = RSM_M)
  return(itemfit_M)
}

IRT_R(M7_f7)


IRT_R_G <- function(x) {
  M <- x-1
  RSM_M <- RSM(X = M)
  person.parameter_M <- person.parameter(object = RSM_M)
  itemfit_M <- itemfit(object = person.parameter_M)
  thresholds_M <- thresholds(object = RSM_M)
  plotICC(object = RSM_M)
}

IRT_R_G(M7_f6)


M7_f6 <- DGS%>%
  select(DGS_21,DGS_65,DGS_1)

M7_f5 <- DGS%>%
  select(DGS_2,DGS_11,DGS_48)


IRT_R(DGS)


M<- M7_f6-1

RSM_M <- RSM(X = M)

person.parameter_M <- person.parameter(object = RSM_M)

itemfit_M <- itemfit(object = person.parameter_M)

itemfit_M

thresholds_M <- thresholds(object = RSM_M)

thresholds_M 

v_threshpar <- thresholds_M$threshpar[1:2]
plotICC(object = RSM_M)
abline(v=v_threshpar,
       col = "gray")

M<- M7_f5-1

RSM_M <- RSM(X = M)

person.parameter_M <- person.parameter(object = RSM_M)

itemfit_M <- itemfit(object = person.parameter_M)

itemfit_M

thresholds_M <- thresholds(object = RSM_M)

thresholds_M 

v_threshpar <- thresholds_M$threshpar[1:2]
plotICC(object = RSM_M)
abline(v=v_threshpar,
       col = "gray")

##create the item information curve

```

###Code Book

```{r codebook}
#codebook_data <- data %>% #select only the variables to display for codebook
#select(-ID)

metadata(data)$name <- "A short name for the data"
metadata(data)$description <- "Longer description of the dataset"
metadata(data)$temporalCoverage <- "Tdataspan of data collection" 

codebook(data, metadata_json = F, metadata_table = F) #generate codebook, excluding some messy meta-data
```