EFA_fit_6
print(EFA_fit_6$loadings, cutoff = 0.4)
print(EFA_fit_6$loadings, cutoff = 0.4)
print(EFA_fit_6$loadings)
DGS_beh <- data%>%
select(c(DGS_13,DGS_73))
EFA_fit_6$rms  # Root mean square of the residuals (lower the better)
EFA_fit_6$RMSEA # root mean squared error of approximation (lower the better)
EFA_fit_6$RMSEA # root mean squared error of approximation (lower the better)
EFA_fit_6$TLI  # tucker lewis index
1- ((EFA_fit_6$STATISTIC-EFA_fit_6$dof)/
(EFA_fit_6$null.chisq-EFA_fit_6$null.dof))  #CFI
# try to trim out behavioral items and create 6 factors
DGS_fit_6C <- DGS %>%
select(c(
DGS_4,DGS_12,DGS_15,DGS_17,DGS_18,DGS_19,DGS_22,DGS_30,DGS_32,DGS_33,DGS_34,DGS_36,DGS_37,DGS_49,DGS_57,DGS_58,DGS_60,DGS_61,DGS_63,DGS_74,DGS_74,
DGS_5,DGS_10,DGS_26,DGS_41,DGS_54,DGS_69,DGS_72,
DGS_9,DGS_20,DGS_27,DGS_29,DGS_46,DGS_52,
DGS_2,DGS_11,DGS_28,DGS_35,DGS_48,DGS_68,
DGS_1,DGS_21,DGS_44,DGS_65,
DGS_6,DGS_38,DGS_39,DGS_45,DGS_67,DGS_70,DGS_71
))
# 64 59 47(poor conceptual fit) 55(mixed conceptual fit) 23(poor conceptual fit)
EFA_fit_6C <- fa(DGS_fit_6C,
nfactors = 6,
rotate = "oblimin",
fm="ml")
EFA_fit_6C
#fit
EFA_fit_6C$rms  # Root mean square of the residuals (lower the better)
EFA_fit_6C$RMSEA # root mean squared error of approximation (lower the better)
EFA_fit_6C$TLI  # tucker lewis index
1- ((EFA_fit_6C$STATISTIC-EFA_fit_6C$dof)/
(EFA_fit_6C$null.chisq-EFA_fit_6C$null.dof))  #CFI
EFA_fit_6C$RMSEA # root mean squared error of approximation (lower the better)
# try to trim out behavioral items and create 6 factors
DGS_fit_6C <- DGS %>%
select(c(
DGS_4,DGS_12,DGS_15,DGS_17,DGS_18,DGS_19,DGS_22,DGS_30,DGS_32,DGS_33,DGS_34,DGS_36,DGS_37,DGS_49,DGS_57,DGS_58,DGS_60,DGS_61,DGS_63,DGS_74,DGS_74,
DGS_5,DGS_10,DGS_26,DGS_41,DGS_54,DGS_69,DGS_72,
DGS_9,DGS_20,DGS_27,DGS_29,DGS_46,DGS_52,
DGS_2,DGS_11,DGS_28,DGS_35,DGS_48,DGS_68,
DGS_1,DGS_21,DGS_44,DGS_65,
DGS_6,DGS_38,DGS_39,DGS_45,DGS_67,DGS_70,DGS_71
))
# 64 59 47(poor conceptual fit) 55(mixed conceptual fit) 23(poor conceptual fit)
EFA_fit_6C <- fa(DGS_fit_6C,
nfactors = 6,
rotate = "oblimin",
fm="ml")
EFA_fit_6C
cor.plot(DGSf_HEXACO)
summary(m_DGS_beh)
m_DGS_beh <- lm (
DGS_73 ~
DGS_6C_1 +
DGS_6C_2 +
DGS_6C_4 +
DGS_6C_3 +
DGS_6C_5 +
DGS_6C_6,
data= data
)
summary(m_DGS_beh)
m_DGS_beh <- lm (
DGS_13 ~
DGS_6C_1 +
DGS_6C_2 +
DGS_6C_4 +
DGS_6C_3 +
DGS_6C_5 +
DGS_6C_6,
data= data
)
0.3196^(1/2)
summary(m_DGS_beh)
cor.plot(DGSf_HEXACO)
m_DGS_beh <- lm (
DGS_donation ~
DGS_6C_1 +
DGS_6C_2 +
DGS_6C_4 +
DGS_6C_3 +
DGS_6C_5 +
DGS_6C_6,
data= data
)
class(data$DGS_donation)
data$DGS_donation <- apply(data$DGS_donation, 2, as.numeric)
data$DGS_vol <- apply(data$DGS_vol, 2, as.numeric)
m_DGS_beh <- lm (
DGS_donation ~
DGS_6C_1 +
DGS_6C_2 +
DGS_6C_4 +
DGS_6C_3 +
DGS_6C_5 +
DGS_6C_6,
data= data
)
summary(m_DGS_beh)
m_DGS_beh <- lm (
DGS_vol ~
DGS_6C_1 +
DGS_6C_2 +
DGS_6C_4 +
DGS_6C_3 +
DGS_6C_5 +
DGS_6C_6,
data= data
)
summary(m_DGS_vol)
summary(m_DGS_beh)
cor.plot(DGSf_HEXACO)
psych::omega(HEXACO_C, nfactors = 4) # alpha = 0.82
DGS_IRT6_1 = mirt(data = DGS_6model_1,
model = 1, #this is for one factor
itemtype = "gpcmIRT" #generalized partial credit model
)
summary(DGS_IRT6_1)
#6 factors model without behavioral items
DGS_6model_1 <- DGS %>%
select(c(DGS_4,DGS_12,DGS_15,DGS_17,DGS_18,DGS_19,DGS_22,DGS_30,DGS_32,DGS_33,DGS_34,DGS_36,DGS_37,DGS_49,DGS_57,DGS_58,DGS_60,DGS_61,DGS_63,DGS_74,DGS_75))
DGS_IRT6_1 = mirt(data = DGS_6model_1,
model = 1, #this is for one factor
itemtype = "gpcmIRT" #generalized partial credit model
)
summary(DGS_IRT6_1)
DGS_IRT6_2 = mirt(data = DGS_6model_2,
model = 1, #this is for one factor
itemtype = "gpcmIRT" #generalized partial credit model
)
summary(DGS_IRT6_2)
DGS_IRT6_3 = mirt(data = DGS_6model_3,
model = 1, #this is for one factor
itemtype = "gpcmIRT" #generalized partial credit model
)
summary(DGS_IRT6_3)
DGS_IRT6_4 = mirt(data = DGS_6model_4,
model = 1, #this is for one factor
itemtype = "gpcmIRT" #generalized partial credit model
)
summary(DGS_IRT6_4)
DGS_IRT6_5 = mirt(data = DGS_6model_5,
model = 1, #this is for one factor
itemtype = "gpcmIRT" #generalized partial credit model
)
summary(DGS_IRT6_5)
DGS_IRT6_6 = mirt(data = DGS_6model_6,
model = 1, #this is for one factor
itemtype = "gpcmIRT" #generalized partial credit model
)
summary(DGS_IRT6_6)
m_DGS_beh <- lm (
DGS_donation ~
DGS_6C_1 +
DGS_6C_2 +
DGS_6C_4 +
DGS_6C_3 +
DGS_6C_5 +
DGS_6C_6,
data= data
)
summary(m_DGS_beh)
cor.plot(DGSf_HEXACO)
knitr::opts_chunk$set(echo = TRUE)
library(corrplot)
corrplot(my_cor, method = "color", type = "lower",
col = colorRampPalette(c("white", "blue"))(100))
corrplot(m_DGS_beh, method = "color", type = "lower",
col = colorRampPalette(c("white", "blue"))(100))
corrplot(DGSf_HEXACO, method = "color", type = "lower",
col = colorRampPalette(c("white", "blue"))(100))
corrplot(DGSf_HEXACO, method = "color", type = "lower",
col = colorRampPalette(c("white", "blue")))
corrplot(DGSf_HEXACO, method = "color", type = "lower")
?corrplot(DGSf_HEXACO, method = "color", type = "lower",
col = colorRampPalette(c("white", "blue")))
corrplot(cor(DGSf_HEXACO), method = "color", type = "lower",
col = colorRampPalette(c("white", "blue")))
corrplot(cor(DGSf_HEXACO), method = "color", type = "lower",
col = colorRampPalette(c("white", "blue"))(100))
corrplot(cor(DGSf_HEXACO), method = "color", type = "lower",
col = colorRampPalette(c("#FDC314", "#9E7E38"))(100))
corrplot(cor(DGSf_HEXACO), method = "color", type = "lower",
col = colorRampPalette(c("#000", "#9E7E38"))(100))
corrplot(cor(DGSf_HEXACO), method = "color", type = "lower",
col = colorRampPalette(c("#000000", "#9E7E38"))(100))
corrplot(cor(DGSf_HEXACO), method = "color", type = "lower",
col = colorRampPalette(c("#53565A", "#9E7E38"))(100))
corrplot(cor(DGSf_HEXACO), method = c("color","number"), type = "lower",
col = colorRampPalette(c("#53565A", "#9E7E38"))(100))
corrplot(cor(DGSf_HEXACO), method = c(number"), type = "lower",
corrplot(cor(DGSf_HEXACO), method = c('number"), type = "lower",
col = colorRampPalette(c("#53565A", "#9E7E38"))(100))
m_DGS_beh <- lm (
DGS_donation ~
DGS_6C_1 +
DGS_6C_2 +
DGS_6C_4 +
DGS_6C_3 +
DGS_6C_5 +
DGS_6C_6,
data= data
)
summary(m_DGS_beh)
tab_model(m_DGS_beh)
m_DGS_beh <- lm (
DGS_vol ~
DGS_6C_1 +
DGS_6C_2 +
DGS_6C_4 +
DGS_6C_3 +
DGS_6C_5 +
DGS_6C_6,
data= data
)
corrplot(cor(DGSf_HEXACO), method = c("number"), type = "lower",
col = colorRampPalette(c("#53565A", "#9E7E38"))(100))
corrplot(cor(DGSf_HEXACO), method = c("number"), type = "lower",
col = colorRampPalette(c("#53565A", "#9E7E38"))(100))
corrplot(cor(DGSf_HEXACO), method = c("number"), type = "lower",
col = colorRampPalette(c("#53565A", "#9E7E38"))(100))
corrplot(cor(DGSf_HEXACO), method = c("color"), type = "lower",
col = colorRampPalette(c("#53565A", "#9E7E38"))(100))
corrplot(cor(DGSf_HEXACO), method = c("color"), type = "lower",
col = colorRampPalette(c("#53565A", "#9E7E38"))(100))
corrplot(cor(DGSf_HEXACO), method = c("color"), type = "lower",
col = colorRampPalette(c("#000000", "#9E7E38"))(100))
corrplot(cor(DGSf_HEXACO), method = "color", type = "lower",
col = colorRampPalette(c("#000000", "#9E7E38"))(100))
cor.plot(DGSf_HEXACO,
main = "correlation table with 6 factors and outcome variables")
knitr::opts_chunk$set(warning = FALSE)
library(summarytools)
library(tidyverse) #data wrangling
library(codebook) #codebook generation
library(future) #reliability
library(ufs) #reliability
library(GGally) #reliability
library(GPArotation) #reliability
library(rio) #reading in different file types
library(labelled) #labeling data
library(psych)
library(corrplot)
library("psych")
library(mirt)
library(eRm)
library(mice)
library(lavaan)
library(semPlot)
library(parameters)
library(broom)
library(likert)
library(sjPlot)
#if need to download the epmr package
#if (!require("devtools")) install.packages("devtools")
#devtools::install_github("talbano/epmr")
## use other read functions as appropriate for file type
dict <- rio::import(file = "DGS_S1_dictionary.xlsx") #dictionary
data <- read.csv(file = 'DGS_S1_Deidentified For Analyses.csv', sep = ",") #data
##trim based on completion time (>180)
data <- data %>%
filter(Duration..in.seconds. >= 180)
data <- data[-c(1:2),-c(1:7)]
## Variable types
names <- dict %>%
filter(type == "character") %>%
pull(variable)
data[,names] <-
lapply(data[,names], as.character)
names <- dict %>%
filter(type == "factor") %>%
pull(variable)
data[,names] <-
lapply(data[,names], as.numeric) #factor variables are coded as numeric for codebook purposes
names <- dict %>%
filter(type == "numeric") %>%
pull(variable)
data[,names] <-
lapply(data[,names], as.numeric)
rm(names)
##data completion check and imputation
#remove failed attention check -55 participants from this process
data <- data %>%
filter(DGS_31 == 4) %>%
filter((DGS_53 == 2))
#prepare dataframe for different scales
DGS <- data[grep("DGS_1",colnames(data)):grep("DGS_75",colnames(data))] %>%
select(!c(DGS_31,DGS_53))
##remove people that gave 90% of the same answer in DGS
for(i in 1:7){
percent <- function(x){
sum((x == i)/length(x)*100)
}
number <- apply(DGS,1,percent)
data <- data[c(number<90),]
}
#function for checking percentage of missing data (unit=%)
percent_missing <- function(x){
sum(is.na((x))/length(x)*100)
}
missing_R <- apply(data,1,percent_missing)
table(missing_R)
#use if don't want imputation(turn off if I need imputation)
replace_rows <- subset(data, missing_R<=0)
data <- replace_rows
## use other read functions as appropriate for file type
dict <- rio::import(file = "DGS_S1_dictionary.xlsx") #dictionary
data <- read.csv(file = 'DGS_S1_Deidentified For Analyses.csv', sep = ",") #data
##trim based on completion time (>180)
data <- data %>%
filter(Duration..in.seconds. >= 180)
data <- data[-c(1:2),-c(1:7)]
## Variable types
names <- dict %>%
filter(type == "character") %>%
pull(variable)
data[,names] <-
lapply(data[,names], as.character)
names <- dict %>%
filter(type == "factor") %>%
pull(variable)
data[,names] <-
lapply(data[,names], as.numeric) #factor variables are coded as numeric for codebook purposes
names <- dict %>%
filter(type == "numeric") %>%
pull(variable)
data[,names] <-
lapply(data[,names], as.numeric)
rm(names)
##data completion check and imputation
#remove failed attention check -55 participants from this process
data <- data %>%
filter(DGS_31 == 4) %>%
filter((DGS_53 == 2))
#prepare dataframe for different scales
DGS <- data[grep("DGS_1",colnames(data)):grep("DGS_75",colnames(data))] %>%
select(!c(DGS_31,DGS_53))
##remove people that gave 90% of the same answer in DGS
for(i in 1:7){
percent <- function(x){
sum((x == i)/length(x)*100)
}
number <- apply(DGS,1,percent)
data <- data[c(number<90),]
}
#function for checking percentage of missing data (unit=%)
percent_missing <- function(x){
sum(is.na((x))/length(x)*100)
}
missing_R <- apply(data,1,percent_missing)
table(missing_R)
#use if don't want imputation(turn off if I need imputation)
replace_rows <- subset(data, missing_R<=0)
data <- replace_rows
##recode
#DGS recode
likert <- dict %>%
filter (value_label == "1 = Strongly disagree, 2 = Disagree, 3 = Somewhat disagree, 4 = Neither agree nor disagree, 5 = Somewhat agree, 6 = Agree, 7 = Strongly agree") %>%
pull(variable)
add_likert <- function(x) {
val_labels(x) <- c("Strongly disagree"= 1, "Disagree" = 2, "Somewhat disagree" = 3, "Neither agree nor disagree" = 4, "Somewhat agree" = 5, "Agree" = 6, "Strongly agree" = 7)
x
}
data <- data %>%
mutate_at(likert,
add_likert)
rm(likert, add_likert)
#HEXACO_C
likert <- dict %>%
filter (value_label == "1 = Strongly disagree, 2 = Somewhat disagree, 3 = Neither agree nor disagree, 4 = Somewhat agree, 5 = Strongly agree") %>%
pull(variable)
add_likert <- function(x) {
val_labels(x) <- c("Strongly disagree"= 1, "Somewhat disagree" = 2, "Neither agree nor disagree" = 3, "Somewhat agree" = 4, "Strongly agree" = 5)
x
}
data <- data %>%
mutate_at(likert,
add_likert)
rm(likert, add_likert)
## Reverse-scoring
reversed_items <- dict %>%  #make a list of reversed items
filter (keying == -1) %>%
pull(variable)
data <- data %>%  #reverse values in data
mutate_at(reversed_items,
reverse_labelled_values)
rm(reversed_items)
##scale construction
## Variable labels
var_label(data) <- dict %>%
select(variable, label) %>%
dict_to_list()
rm(extra)
##DGS
DGS <- dict %>%
filter (scale == "DGS") %>%
pull(variable)
data$DGS <- data %>%
select(all_of(DGS)) %>%
aggregate_and_document_scale()
##HEXACO_C
HEXACO_C <- dict %>%
filter (scale == "HEXACO_C") %>%
pull(variable)
data$HEXACO_C <- data %>%
select(all_of(HEXACO_C)) %>%
aggregate_and_document_scale()
###7 factors model
M7_1 <- dict %>%
filter (M7_concise == 1) %>%
pull(variable)
M7_2 <- dict %>%
filter (M7_concise == 2) %>%
pull(variable)
M7_3 <- dict %>%
filter (M7_concise == 3) %>%
pull(variable)
M7_4 <- dict %>%
filter (M7_concise == 4) %>%
pull(variable)
M7_5 <- dict %>%
filter (M7_concise == 5) %>%
pull(variable)
M7_6 <- dict %>%
filter (M7_concise == 6) %>%
pull(variable)
M7_7 <- dict %>%
filter (M7_concise == 7) %>%
pull(variable)
###7 factors model
data$M7_1 <- data %>%
select(all_of(M7_1)) %>%
aggregate_and_document_scale()
data$M7_2 <- data %>%
select(all_of(M7_2)) %>%
aggregate_and_document_scale()
data$M7_3 <- data %>%
select(all_of(M7_3)) %>%
aggregate_and_document_scale()
data$M7_4 <- data %>%
select(all_of(M7_4)) %>%
aggregate_and_document_scale()
data$M7_5 <- data %>%
select(all_of(M7_5)) %>%
aggregate_and_document_scale()
data$M7_6 <- data %>%
select(all_of(M7_6)) %>%
aggregate_and_document_scale()
data$M7_7 <- data %>%
select(all_of(M7_7)) %>%
aggregate_and_document_scale()
## assumption check
random_variable <- rchisq(nrow(data), 7)
fake_model <- lm(random_variable ~ .,
data = data[ , -c(86:91)])
standardized <- rstudent(fake_model)
fitvalues <- scale(fake_model$fitted.values)
plot(fake_model,2)#check for linearity
#We assume the the multivariate relationship between continuous variables is linear (i.e., no curved)
#There are many ways to test this, but we can use a QQ/PP Plot to examine for linearity
hist(standardized)#check for normality
#We expect that the residuals are normally distributed
#Not that the *sample* is normally distributed
#Generally, SEM requires a large sample size, thus, buffering against normality deviations
{plot(standardized, fitvalues)
abline(v = 0)
abline(h = 0)
}#check for homogeneity + Homoscedasticity
#These assumptions are about equality of the variances
#We assume equal variances between groups for things like t-tests, ANOVA
#Here the assumption is equality in the spread of variance across predicted values
##rename
data_cleaned <- data
#prepare dataframe for different scales
DGS <- data_cleaned[grep("DGS_1",colnames(data_cleaned)):grep("DGS_75",colnames(data_cleaned))]
DGS <- DGS[,-c(grep("DGS_31",colnames(DGS)),grep("DGS_53",colnames(DGS)))]
HEXACO_C <- data_cleaned[grep("HEXACO_C_1",colnames(data_cleaned)):grep("HEXACO_C_10",colnames(data_cleaned))]
View(data)
knitr::opts_chunk$set(warning = FALSE)
# try to trim out behavioral items and create 6 factors
DGS_fit_6C <- DGS %>%
select(c(
DGS_12,DGS_15,DGS_17,DGS_18,DGS_19,DGS_30,DGS_32,DGS_33,DGS_34,DGS_36,DGS_56,DGS_49,DGS_57,DGS_58,DGS_60,DGS_61,DGS_63,DGS_75,
DGS_5,DGS_10,DGS_26,DGS_41,DGS_54,DGS_69,DGS_72,
DGS_9,DGS_27,DGS_29,DGS_46,DGS_52,
DGS_28,DGS_35,DGS_48,DGS_68,
DGS_21,DGS_44,DGS_65,
DGS_6,DGS_38,DGS_39,DGS_70,DGS_71
))
EFA_fit_6C <- fa(DGS_fit_6C,
nfactors = 6,
rotate = "oblimin",
fm="ml")
EFA_fit_6C <- fa.sort(EFA_fit_6C)
EFA_fit_6C
print(EFA_fit_6C$loadings, cutoff = 0.3)
print(EFA_fit_6C$loadings)
# the current model is based on the 6 factors solution in Portfolios_4
# trim 67,74 because of the poor fit in factor 6 (<0.4)
#fit
EFA_fit_6C$rms  # Root mean square of the residuals (lower the better)
EFA_fit_6C$RMSEA # root mean squared error of approximation (lower the better)
EFA_fit_6C$TLI  # tucker lewis index
1- ((EFA_fit_6C$STATISTIC-EFA_fit_6C$dof)/
(EFA_fit_6C$null.chisq-EFA_fit_6C$null.dof))  #CFI
print(EFA_fit_6C$loadings)
---
title: "Portfolios_5"
knitr::opts_chunk$set(echo = TRUE)
?tab_model(m_DGS_beh)
